{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicolaGabriele/progettoDeepLearning/blob/main/ProgettoDeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8qVmn0NluI3",
        "outputId": "b155b18e-30cf-4b37-8abd-1949456f4809"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "dW-CZdpb_BPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c2a1095-0307-4f52-d592-fe127fe63673"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.9.12-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.9.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awWHkUZ2-BrY",
        "outputId": "31493b8f-9a71-43e3-bb4c-e1492d968a43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda is selected !\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torchsummary import summary\n",
        "from sklearn.metrics import f1_score\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models as resnet_model\n",
        "import torchvision\n",
        "from torcheval.metrics import BinaryF1Score\n",
        "import os\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(str(device) + ' is selected !')\n",
        "\n",
        "IMAGES_DIR = '/content/drive/MyDrive/machinedeeplearning/BUS_UC/All/images'\n",
        "LABELS_DIR = '/content/drive/MyDrive/machinedeeplearning/BUS_UC/All/masks'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Dataset Loading***"
      ],
      "metadata": {
        "id": "RmdXcmqwlFSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, img_dir, lab_dir, train = True):\n",
        "    super(Dataset, self).__init__()\n",
        "    self.img_dir = img_dir\n",
        "    self.lab_dir = lab_dir\n",
        "    self.img_names = os.listdir(img_dir)\n",
        "    self.lab_names = os.listdir(lab_dir)\n",
        "    self.transform1 = torchvision.transforms.Compose(\n",
        "        [torchvision.transforms.Resize((224,224)),\n",
        "        torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
        "        torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
        "        torchvision.transforms.GaussianBlur(3)]\n",
        "    )\n",
        "    self.transform = torchvision.transforms.Resize((224,224))\n",
        "    self.train = train\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    img_name = os.path.join(self.img_dir,self.img_names[idx])\n",
        "    mask_name = os.path.join(self.lab_dir, self.lab_names[idx])\n",
        "    image = torchvision.io.read_image(img_name)/255\n",
        "    mask = torchvision.io.read_image(mask_name)/255\n",
        "    if self.train:\n",
        "      image = self.transform1(image)\n",
        "      mask = self.transform1(mask)\n",
        "    else:\n",
        "      image = self.transform(image)\n",
        "      mask = self.transform(mask)\n",
        "    return (image,mask[0])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(os.listdir(self.img_dir))"
      ],
      "metadata": {
        "id": "S7axHm52lJa0"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = Dataset(IMAGES_DIR,LABELS_DIR,train=False)\n",
        "validationset, testset, trainset = random_split(data, [0.15,0.15,0.7])"
      ],
      "metadata": {
        "id": "aX_r9vP-MdPS"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***MET-NET***"
      ],
      "metadata": {
        "id": "e6uPGnVVk8mM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBottleneckLayer(nn.Module):\n",
        "    def __init__(self, in_channels, n_filters, use_transpose=True):\n",
        "        super(DecoderBottleneckLayer, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels // 4, 1)\n",
        "        self.norm1 = nn.BatchNorm2d(in_channels // 4)\n",
        "        self.relu1 = nn.LeakyReLU()\n",
        "\n",
        "        if use_transpose:\n",
        "            self.up = nn.Sequential(\n",
        "                nn.ConvTranspose2d(\n",
        "                    in_channels // 4, in_channels // 4, 3, stride=2, padding=1, output_padding=1\n",
        "                ),\n",
        "                nn.BatchNorm2d(in_channels // 4),\n",
        "                nn.LeakyReLU()\n",
        "            )\n",
        "        else:\n",
        "            self.up = nn.Upsample(scale_factor=2, align_corners=True, mode=\"bilinear\")\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels // 4, n_filters, 1)\n",
        "        self.norm3 = nn.BatchNorm2d(n_filters)\n",
        "        self.relu3 = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.up(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.norm3(x)\n",
        "        x = self.relu3(x)\n",
        "        return x\n",
        "\n",
        "class FFBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(FFBlock, self).__init__()\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, padding=1)\n",
        "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=1)\n",
        "\n",
        "        self.relu3 = nn.LeakyReLU()\n",
        "        self.relu1 = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x3 = self.conv3(x)\n",
        "        x3 = self.relu3(x3)\n",
        "        x1 = self.conv1(x)\n",
        "        x1 = self.relu1(x1)\n",
        "        out = x3 + x1\n",
        "\n",
        "        return out\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channel, r=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // r, bias=False),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(channel // r, channel, bias=False),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        ## Squeeze operation\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        ## Excitation operation\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        ## Fusion operation\n",
        "        y = torch.mul(x, y)\n",
        "        return y\n",
        "\n",
        "class PDFBlock(nn.Module):\n",
        "    def __init__(self,in_channels, out_channels_list, kernel_size_list, dilation_list):\n",
        "        super(PDFBlock, self).__init__()\n",
        "        self.conv_num = len(out_channels_list)\n",
        "        assert(self.conv_num == 4)\n",
        "        assert(self.conv_num == len(kernel_size_list) and self.conv_num == len(dilation_list))\n",
        "        pad0 = int((kernel_size_list[0] - 1) / 2 * dilation_list[0])\n",
        "        pad1 = int((kernel_size_list[1] - 1) / 2 * dilation_list[1])\n",
        "        pad2 = int((kernel_size_list[2] - 1) / 2 * dilation_list[2])\n",
        "        pad3 = int((kernel_size_list[3] - 1) / 2 * dilation_list[3])\n",
        "        self.conv_1 = nn.Conv2d(in_channels, out_channels_list[0], kernel_size = kernel_size_list[0], dilation = dilation_list[0], padding = pad0 )\n",
        "        self.conv_2 = nn.Conv2d(in_channels, out_channels_list[1], kernel_size = kernel_size_list[1], dilation = dilation_list[1], padding = pad1 )\n",
        "        self.conv_3 = nn.Conv2d(in_channels, out_channels_list[2], kernel_size = kernel_size_list[2], dilation = dilation_list[2], padding = pad2 )\n",
        "        self.conv_4 = nn.Conv2d(in_channels, out_channels_list[3], kernel_size = kernel_size_list[3], dilation = dilation_list[3], padding = pad3 )\n",
        "\n",
        "        out_channels  = out_channels_list[0] + out_channels_list[1] + out_channels_list[2] + out_channels_list[3]\n",
        "        self.conv_1x1 = nn.Sequential(\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=1, padding=0),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv_1(x)\n",
        "        x2 = self.conv_2(x)\n",
        "        x3 = self.conv_3(x)\n",
        "        x4 = self.conv_4(x)\n",
        "\n",
        "        y  = torch.cat([x1, x2, x3, x4], dim=1)\n",
        "        y  = self.conv_1x1(y)\n",
        "        return y\n",
        "\n",
        "class MET_Net(nn.Module):\n",
        "    def __init__(self, n_channels=3, n_classes=1):\n",
        "        super(MET_Net, self).__init__()\n",
        "\n",
        "        transformer = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_distilled_patch16_224', pretrained=True)\n",
        "        resnet = resnet_model.resnet18(pretrained=True)\n",
        "\n",
        "        self.firstconv = resnet.conv1\n",
        "        self.firstbn = resnet.bn1\n",
        "        self.firstrelu = resnet.relu\n",
        "\n",
        "        self.encoder1 = resnet.layer1\n",
        "        self.encoder2 = resnet.layer2\n",
        "        self.encoder3 = resnet.layer3\n",
        "        self.encoder4 = resnet.layer4\n",
        "\n",
        "        self.patch_embed = transformer.patch_embed\n",
        "        self.transformers = nn.ModuleList( [transformer.blocks[i] for i in range(12)] )\n",
        "\n",
        "        self.conv_seq_img = nn.Conv2d(in_channels=192, out_channels=512, kernel_size=1, padding=0)\n",
        "        self.se = SEBlock(channel=1024)\n",
        "        self.conv2d = nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1, padding=0)\n",
        "\n",
        "        self.FFBlock1 = FFBlock(channels=64)\n",
        "        self.FFBlock2 = FFBlock(channels=128)\n",
        "        self.FFBlock3 = FFBlock(channels=256)\n",
        "\n",
        "        self.FFB1 = nn.ModuleList([self.FFBlock1 for i in range(6)])\n",
        "        self.FFB2 = nn.ModuleList([self.FFBlock2 for i in range(4)])\n",
        "        self.FFB3 = nn.ModuleList([self.FFBlock3 for i in range(2)])\n",
        "\n",
        "        filters = [64, 128, 256, 512]\n",
        "\n",
        "        self.decoder4 = DecoderBottleneckLayer(filters[3], filters[2])\n",
        "        self.decoder3 = DecoderBottleneckLayer(filters[2], filters[1])\n",
        "        self.decoder2 = DecoderBottleneckLayer(filters[1], filters[0])\n",
        "        self.decoder1 = DecoderBottleneckLayer(filters[0], filters[0])\n",
        "\n",
        "        self.final_conv1 = nn.ConvTranspose2d(filters[0], 32, 4, 2, 1)\n",
        "        self.final_relu1 = nn.LeakyReLU()\n",
        "        self.final_conv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "        self.final_relu2 = nn.LeakyReLU()\n",
        "        self.final_conv3 = nn.Conv2d(32, n_classes, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "\n",
        "        e0 = self.firstconv(x)\n",
        "        e0 = self.firstbn(e0)\n",
        "        e0 = self.firstrelu(e0)\n",
        "\n",
        "        e1 = self.encoder1(e0)\n",
        "        e2 = self.encoder2(e1)\n",
        "        e3 = self.encoder3(e2)\n",
        "        feature_cnn = self.encoder4(e3)\n",
        "\n",
        "        emb = self.patch_embed(x)\n",
        "        for i in range(12):\n",
        "            emb = self.transformers[i](emb)\n",
        "\n",
        "        feature_tf = emb.permute(0, 2, 1)\n",
        "        feature_tf = feature_tf.view(b, 192, 14, 14)\n",
        "        feature_tf = self.conv_seq_img(feature_tf)\n",
        "\n",
        "        feature_cat = torch.cat((feature_cnn, feature_tf), dim=1)\n",
        "        feature_att = self.se(feature_cat)\n",
        "        feature_out = self.conv2d(feature_att)\n",
        "\n",
        "        for i in range(2):\n",
        "            e3 = self.FFB3[i](e3)\n",
        "        for i in range(4):\n",
        "            e2 = self.FFB2[i](e2)\n",
        "        for i in range(6):\n",
        "            e1 = self.FFB1[i](e1)\n",
        "\n",
        "        d4 = self.decoder4(feature_out) + e3\n",
        "        d3 = self.decoder3(d4) + e2\n",
        "        d2 = self.decoder2(d3) + e1\n",
        "\n",
        "        out1 = self.final_conv1(d2)\n",
        "        out1 = self.final_relu1(out1)\n",
        "\n",
        "        out = self.final_conv2(out1)\n",
        "        out = self.final_relu2(out)\n",
        "        out = self.final_conv3(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "nkRyLfiO-Z6p"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MET_Net()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESzgAzWE-eci",
        "outputId": "b54756e7-77fe-440e-a292-20a18a1c8fbe"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_deit_main\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Training***"
      ],
      "metadata": {
        "id": "PMfo2VGyGMp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiceLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DiceLoss,self).__init__()\n",
        "\n",
        "  def forward(self, predictions, targets):\n",
        "    pred = torch.nn.functional.sigmoid(predictions)\n",
        "    pred = torch.where(pred>=0.7, 1, 0).squeeze(dim=1)\n",
        "    targ = targets.squeeze(dim=1)\n",
        "    s1 = (targ*pred).flatten().sum()\n",
        "    s2 = (pred**2).flatten().sum()\n",
        "    s3 = (targ**2).flatten().sum()\n",
        "    rap = (1+2*s1)/(1+s3+s2)\n",
        "    return torch.tensor(1-rap.mean(), requires_grad=True)"
      ],
      "metadata": {
        "id": "-7u2ANeWPWYh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryDiceLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, smooth=1, p=2, reduction='mean'):\n",
        "        super(BinaryDiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "        self.p = p\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        assert predict.shape[0] == target.shape[0], \"predict & target batch size don't match\"\n",
        "        predict = predict.contiguous().view(predict.shape[0], -1)\n",
        "        target = target.contiguous().view(target.shape[0], -1)\n",
        "\n",
        "        num = torch.sum(torch.mul(predict, target), dim=1) + self.smooth\n",
        "        den = torch.sum(predict.pow(self.p) + target.pow(self.p), dim=1) + self.smooth\n",
        "\n",
        "        loss = torch.tensor(1 - num / den, requires_grad=True)\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        elif self.reduction == 'none':\n",
        "            return loss\n",
        "        else:\n",
        "            raise Exception('Unexpected reduction {}'.format(self.reduction))"
      ],
      "metadata": {
        "id": "NkQbrqM_cFqh"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#iperparameter\n",
        "num_epochs = 100\n",
        "batch_size = 4\n",
        "learning_rate=1e-4\n",
        "optimizer = torch.optim.NAdam(params=model.parameters(),lr=learning_rate)\n",
        "lr = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,patience=15)\n",
        "dataloader = torch.utils.data.DataLoader(trainset,batch_size=batch_size,shuffle=True)\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()#BinaryDiceLoss()"
      ],
      "metadata": {
        "id": "Bxpm_bqzsRqC"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(epoch_index):\n",
        "    l = []\n",
        "\n",
        "    for i, data in enumerate(dataloader):\n",
        "\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to('cuda')\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs).squeeze(dim=1)\n",
        "        #outputs = torch.nn.functional.sigmoid(outputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        #print('Batch {} loss: {:.4f}'.format(i,loss.item()))\n",
        "\n",
        "        l.append(loss.item())\n",
        "\n",
        "    print('Epoch {} Loss (mean): {:.4f}'.format(epoch_index, np.mean(l)))"
      ],
      "metadata": {
        "id": "1YDMRd3pX-P-"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "model.to('cuda')\n",
        "for epoch in range(61,num_epochs+1):\n",
        "  train_one_epoch(epoch)\n",
        "  if epoch%10 == 0:\n",
        "    torch.save(model,f'/content/drive/MyDrive/machinedeeplearning/models/model_{epoch}ep')"
      ],
      "metadata": {
        "id": "2rmulNuJZyU1",
        "outputId": "c051f945-8481-49ca-a78e-b45e75d16bec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 61 Loss (mean): 0.0191\n",
            "Epoch 62 Loss (mean): 0.0286\n",
            "Epoch 63 Loss (mean): 0.0202\n",
            "Epoch 64 Loss (mean): 0.0160\n",
            "Epoch 65 Loss (mean): 0.0146\n",
            "Epoch 66 Loss (mean): 0.0143\n",
            "Epoch 67 Loss (mean): 0.0139\n",
            "Epoch 68 Loss (mean): 0.0137\n",
            "Epoch 69 Loss (mean): 0.0134\n",
            "Epoch 70 Loss (mean): 0.0134\n",
            "Epoch 71 Loss (mean): 0.0128\n",
            "Epoch 72 Loss (mean): 0.0121\n",
            "Epoch 73 Loss (mean): 0.0119\n",
            "Epoch 74 Loss (mean): 0.0623\n",
            "Epoch 75 Loss (mean): 0.0335\n",
            "Epoch 76 Loss (mean): 0.0213\n",
            "Epoch 77 Loss (mean): 0.0148\n",
            "Epoch 78 Loss (mean): 0.0121\n",
            "Epoch 79 Loss (mean): 0.0106\n",
            "Epoch 80 Loss (mean): 0.0101\n",
            "Epoch 81 Loss (mean): 0.0097\n",
            "Epoch 82 Loss (mean): 0.0094\n",
            "Epoch 83 Loss (mean): 0.0092\n",
            "Epoch 84 Loss (mean): 0.0091\n",
            "Epoch 85 Loss (mean): 0.0092\n",
            "Epoch 86 Loss (mean): 0.0090\n",
            "Epoch 87 Loss (mean): 0.0089\n",
            "Epoch 88 Loss (mean): 0.0089\n",
            "Epoch 89 Loss (mean): 0.0089\n",
            "Epoch 90 Loss (mean): 0.0088\n",
            "Epoch 91 Loss (mean): 0.0089\n",
            "Epoch 92 Loss (mean): 0.0089\n",
            "Epoch 93 Loss (mean): 0.0088\n",
            "Epoch 94 Loss (mean): 0.0087\n",
            "Epoch 95 Loss (mean): 0.0086\n",
            "Epoch 96 Loss (mean): 0.0086\n",
            "Epoch 97 Loss (mean): 0.0085\n",
            "Epoch 98 Loss (mean): 0.0086\n",
            "Epoch 99 Loss (mean): 0.0087\n",
            "Epoch 100 Loss (mean): 0.0086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "train_one_epoch(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRkePmdAdGzS",
        "outputId": "6c3584ac-c150-458e-97ff-b441f3c96c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss (mean): 0.4986859874197548\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn((3,3,224,224))\n",
        "model.to('cpu')\n",
        "k = model(k)\n",
        "k = torch.nn.functional.sigmoid(k)\n",
        "k = torch.where(k>=0.6,1,0)\n",
        "l = torch.rand((3,1,224,224))\n",
        "l = torch.where(l>=0.6,1,0)\n",
        "loss_fn(k,l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIgcIm4fiFyR",
        "outputId": "da37be38-ca48-4b44-e7b2-db5b34fb4174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5891)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Model Evaluation***"
      ],
      "metadata": {
        "id": "gPB1JcOqNl9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('/content/drive/MyDrive/machinedeeplearning/models/model_100ep')\n",
        "model.eval()\n",
        "testloader = torch.utils.data.DataLoader(testset,batch_size=1,shuffle=False)\n",
        "m = BinaryF1Score(threshold=0.5)\n",
        "prec = torcheval.metrics.BinaryPrecision(threshold=0.5)\n",
        "rec = torcheval.metrics.BinaryRecall(threshold=0.5)\n",
        "for i, data in enumerate(testloader):\n",
        "  x, ytrue = data\n",
        "  x = x.to(device)\n",
        "  ypred = model(x)\n",
        "  ypred = torch.nn.functional.sigmoid(ypred)[0][0].flatten()\n",
        "  m.update(ypred,ytrue[0].flatten())\n",
        "  prec.update(ypred,ytrue[0].flatten())\n",
        "\n",
        "print(m.compute())\n",
        "print(prec.compute())\n"
      ],
      "metadata": {
        "id": "GcZW2TMDNpFT",
        "outputId": "502d3d14-f521-42e7-88f5-ed130e565be9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.5141)\n",
            "tensor(0.5678)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torcheval\n",
        "ypred = trainset[0][1]\n",
        "ypred = ypred.flatten()\n",
        "m = BinaryF1Score(threshold=0.7)\n",
        "m.update(ypred,ypred)\n",
        "m.compute()"
      ],
      "metadata": {
        "id": "6unavONAl5Zq",
        "outputId": "d6369b37-b356-4791-96b3-b8c8be26b375",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9814)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torcheval"
      ],
      "metadata": {
        "id": "RGb7JQbzm7Eq",
        "outputId": "9a771a8e-fe8f-4db4-8382-47354c74e58a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torcheval in /usr/local/lib/python3.10/dist-packages (0.0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ytrue = ytrue.to('cpu').detach()\n",
        "ypred = ypred.squeeze(dim=1).to('cpu').detach()\n",
        "ypred = torch.nn.functional.sigmoid(ypred)\n",
        "ypred = torch.where(ypred>=0.7,1,0)\n",
        "f1_score(y_true=ytrue,y_pred=ypred)"
      ],
      "metadata": {
        "id": "ngLqYNZaRene",
        "outputId": "d5de637c-7c4d-4e17-d273-9087e326017f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "unknown is not supported",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-db0d43e69671>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mypred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mypred\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mytrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mypred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.66666667\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m        \u001b[0;34m,\u001b[0m \u001b[0;36m0.66666667\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \"\"\"\n\u001b[0;32m-> 1146\u001b[0;31m     return fbeta_score(\n\u001b[0m\u001b[1;32m   1147\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1285\u001b[0m     \"\"\"\n\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m     _, _, f, _ = precision_recall_fscore_support(\n\u001b[0m\u001b[1;32m   1288\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_set_wise_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"average has to be one of \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m     \u001b[0;31m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;31m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# No metrics support \"multiclass-multioutput\" format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: unknown is not supported"
          ]
        }
      ]
    }
  ]
}